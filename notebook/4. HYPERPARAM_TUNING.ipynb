{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37b21a8",
   "metadata": {},
   "source": [
    "##### Hyperparam Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e629d4",
   "metadata": {},
   "source": [
    "Here I will do hyperparameter tuning on the most promising models on the scaled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d6e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "651f9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge,Lasso\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02db8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train=pd.read_csv('data/scaled_X_train.csv')\n",
    "scaled_y_train=pd.read_csv('data/scaled_y_train.csv')\n",
    "scaled_X_test=pd.read_csv('data/scaled_X_test.csv')\n",
    "scaled_y_test=pd.read_csv('data/scaled_y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f460fd28",
   "metadata": {},
   "source": [
    "Define Adj_R2 function and evalute_models function as done in Feature selection notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22599560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true, predicted):\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(true, predicted))\n",
    "    mape = mean_absolute_percentage_error(true, predicted)\n",
    "    r2_square = r2_score(true, predicted)\n",
    "    return mae, mse, rmse, mape, r2_square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511688f4",
   "metadata": {},
   "source": [
    "Define the hyperparam tuning, first broad tuning on best 6 , then more fine tuning for top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9b69049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flight_price_quick_hyperparam_tuning(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Two-stage hyperparameter tuning for airline flight price prediction:\n",
    "    1. Quick tuning on all 6 models to identify top 3\n",
    "    2. Intensive tuning on top 3 performers\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train : Training data and labels\n",
    "    X_test, y_test : Test data and labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df_results_quick : DataFrame with quick tuning results\n",
    "    df_results_intensive : DataFrame with intensive tuning results for top 3\n",
    "    best_models : Dictionary containing the best trained models\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*150)\n",
    "    print(\"STAGE 1: QUICK HYPERPARAMETER TUNING - ALL 6 MODELS\")\n",
    "    print(\"=\"*150)\n",
    "    \n",
    "    # Stage 1: Quick hyperparameter tuning\n",
    "    quick_models = {\n",
    "        \"CatBoost\": {\n",
    "            \"model\": CatBoostRegressor(random_state=42, verbose=0),\n",
    "            \"params\": {\n",
    "                'iterations': [100, 200],\n",
    "                'depth': [4, 6, 8],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'l2_leaf_reg': [1, 3, 5]\n",
    "            }\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"model\": XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'subsample': [0.8, 1.0],\n",
    "                'colsample_bytree': [0.8, 1.0]\n",
    "            }\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            \"model\": RandomForestRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2],\n",
    "                'max_features': ['sqrt', 'log2']\n",
    "            }\n",
    "        },\n",
    "        \"Gradient Boosting\": {\n",
    "            \"model\": GradientBoostingRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'subsample': [0.8, 1.0],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        \"K-Neighbors\": {\n",
    "            \"model\": KNeighborsRegressor(),\n",
    "            \"params\": {\n",
    "                'n_neighbors': [3, 5, 7, 9],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "                'leaf_size': [20, 30, 40]\n",
    "            }\n",
    "        },\n",
    "        \"Decision Tree\": {\n",
    "            \"model\": DecisionTreeRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                'max_depth': [5, 10, 15, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2', None]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_quick = []\n",
    "    trained_models = {}\n",
    "    \n",
    "    # Quick tuning with fewer iterations\n",
    "    print(\"\\nPerforming quick hyperparameter tuning\")\n",
    "    for model_name, model_info in quick_models.items():\n",
    "        print(f\"\\n  Training {model_name}\", end=\" \")\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model_info[\"model\"],\n",
    "            param_distributions=model_info[\"params\"],\n",
    "            n_iter=20,  # Limited iterations for quick assessment\n",
    "            cv=3,  # Fewer folds for speed\n",
    "            scoring='neg_mean_squared_error',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_model = random_search.best_estimator_\n",
    "        trained_models[model_name] = best_model\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        \n",
    "\n",
    "        model_train_mae, model_train_mse, model_train_rmse, model_train_mape, model_train_r2 = evaluate_model(y_train, y_train_pred)\n",
    "        # y_test is actual value, y_test_pred is predicted value from X_test\n",
    "        model_test_mae, model_test_mse, model_test_rmse, model_test_mape, model_test_r2 = evaluate_model(y_test, y_test_pred)\n",
    "        \n",
    "        results_quick.append({\n",
    "            'Model': model_name,\n",
    "            'Train RMSE': model_train_rmse,\n",
    "            'Train MSE': model_train_mse,\n",
    "            'Train MAE': model_train_mae,\n",
    "            'Train MAPE': model_train_mape,\n",
    "            'Train R2': model_train_r2,\n",
    "            'Test RMSE': model_test_rmse,\n",
    "            'Test MSE': model_test_mse,\n",
    "            'Test MAE': model_test_mae,\n",
    "            'Test MAPE': model_test_mape,\n",
    "            'Test R2': model_test_r2,\n",
    "            'Best Params': str(random_search.best_params_)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for quick results\n",
    "    df_results_quick = pd.DataFrame(results_quick)\n",
    "    \n",
    "    # Display quick tuning results\n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"QUICK TUNING RESULTS - ALL MODELS\")\n",
    "    print(\"=\"*150)\n",
    "    display_df = df_results_quick.drop('Best Params', axis=1)\n",
    "    print(display_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    \n",
    "    # Identify top 3 models based on Test R2\n",
    "    top_3_models = df_results_quick.nlargest(3, 'Test R2')['Model'].tolist()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"TOP 3 MODELS SELECTED FOR INTENSIVE TUNING\")\n",
    "    print(\"=\"*150)\n",
    "    for i, model_name in enumerate(top_3_models, 1):\n",
    "        model_stats = df_results_quick[df_results_quick['Model'] == model_name].iloc[0]\n",
    "        print(f\"{i}. {model_name}\")\n",
    "        print(f\"   - Test R2: {model_stats['Test R2']:.4f}\")\n",
    "        print(f\"   - Test RMSE: {model_stats['Test RMSE']:.4f}\")\n",
    "        print(f\"   - Test MAE: {model_stats['Test MAE']:.4f}\")\n",
    "    \n",
    "    return df_results_quick, trained_models, top_3_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b89f5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flight_price_complete_hyperparam_tuning(X_train, y_train, X_test, y_test, top_3_models, df_results_quick):    # Stage 2: Intensive hyperparameter tuning for top 3\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"STAGE 2: Complete HYPERPARAMETER TUNING - TOP 3 MODELS\")\n",
    "    print(\"=\"*150)\n",
    "    \n",
    "    # Intensive parameter grids\n",
    "    intensive_models = {\n",
    "        \"CatBoost\": {\n",
    "            \"model\": CatBoostRegressor(random_state=42, verbose=0),\n",
    "            \"params\": {\n",
    "                'iterations': [100, 200, 300, 500],\n",
    "                'depth': [4, 6, 8, 10],\n",
    "                'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15],\n",
    "                'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "                'border_count': [32, 64, 128],\n",
    "                'subsample': [0.7, 0.8, 0.9, 1.0]\n",
    "            }\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"model\": XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200, 300, 500],\n",
    "                'max_depth': [3, 5, 7, 9, 11],\n",
    "                'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15],\n",
    "                'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                'gamma': [0, 0.1, 0.3, 0.5, 1],\n",
    "                'min_child_weight': [1, 3, 5, 7],\n",
    "                'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "                'reg_lambda': [0.1, 1, 10]\n",
    "            }\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            \"model\": RandomForestRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200, 300, 500],\n",
    "                'max_depth': [10, 15, 20, 25, 30, None],\n",
    "                'min_samples_split': [2, 5, 10, 15],\n",
    "                'min_samples_leaf': [1, 2, 4, 6],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'bootstrap': [True, False],\n",
    "                'max_samples': [0.7, 0.8, 0.9, None]\n",
    "            }\n",
    "        },\n",
    "        \"Gradient Boosting\": {\n",
    "            \"model\": GradientBoostingRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200, 300, 500],\n",
    "                'max_depth': [3, 5, 7, 9, 11],\n",
    "                'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15],\n",
    "                'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                'min_samples_split': [2, 5, 10, 15],\n",
    "                'min_samples_leaf': [1, 2, 4, 6],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'loss': ['squared_error', 'absolute_error', 'huber']\n",
    "            }\n",
    "        },\n",
    "        \"K-Neighbors\": {\n",
    "            \"model\": KNeighborsRegressor(),\n",
    "            \"params\": {\n",
    "                'n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                'leaf_size': [10, 20, 30, 40, 50],\n",
    "                'p': [1, 2],\n",
    "                'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "            }\n",
    "        },\n",
    "        \"Decision Tree\": {\n",
    "            \"model\": DecisionTreeRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                'max_depth': [5, 10, 15, 20, 25, 30, None],\n",
    "                'min_samples_split': [2, 5, 10, 15, 20],\n",
    "                'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'splitter': ['best', 'random'],\n",
    "                'min_impurity_decrease': [0.0, 0.01, 0.05, 0.1]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_intensive = []\n",
    "    best_models = {}\n",
    "    \n",
    "    print(\"\\nPerforming intensive hyperparameter tuning on top 3 models\")\n",
    "    for model_name in top_3_models:\n",
    "        if model_name not in intensive_models:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n  Training {model_name}\", end=\" \")\n",
    "        \n",
    "        model_info = intensive_models[model_name]\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model_info[\"model\"],\n",
    "            param_distributions=model_info[\"params\"],\n",
    "            n_iter=50,  # More iterations for intensive search\n",
    "            cv=5,  # More folds for better validation\n",
    "            scoring='neg_mean_squared_error',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_model = random_search.best_estimator_\n",
    "        best_models[model_name] = best_model\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        \n",
    "        model_train_mae, model_train_mse, model_train_rmse, model_train_mape, model_train_r2 = evaluate_model(y_train, y_train_pred)\n",
    "        # y_test is actual value, y_test_pred is predicted value from X_test\n",
    "        model_test_mae, model_test_mse, model_test_rmse, model_test_mape, model_test_r2 = evaluate_model(y_test, y_test_pred)\n",
    "        \n",
    "        # Calculate improvement from quick tuning\n",
    "        quick_r2 = df_results_quick[df_results_quick['Model'] == model_name]['Test R2'].values[0]\n",
    "        r2_improvement = model_test_r2 - quick_r2\n",
    "        \n",
    "        print(\"Completed\")\n",
    "        \n",
    "        results_intensive.append({\n",
    "            'Model': model_name,\n",
    "            'Train RMSE': model_train_rmse,\n",
    "            'Train MSE': model_train_mse,\n",
    "            'Train MAE': model_train_mae,\n",
    "            'Train MAPE': model_train_mape,\n",
    "            'Train R2': model_train_r2,\n",
    "            'Test RMSE': model_test_rmse,\n",
    "            'Test MSE': model_test_mse,\n",
    "            'Test MAE': model_test_mae,\n",
    "            'Test MAPE': model_test_mape,\n",
    "            'Test R2': model_test_r2,\n",
    "            'R2 Improvement': r2_improvement,\n",
    "            'Best Params': str(random_search.best_params_)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for intensive results\n",
    "    df_results_intensive = pd.DataFrame(results_intensive)\n",
    "    \n",
    "    # Display intensive tuning results\n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"Complete TUNING RESULTS - TOP 3 MODELS\")\n",
    "    print(\"=\"*150)\n",
    "    display_df_intensive = df_results_intensive.drop('Best Params', axis=1)\n",
    "    print(display_df_intensive.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    \n",
    "    # Display best parameters for intensive tuning\n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"BEST HYPERPARAMETERS - Complete TUNING\")\n",
    "    print(\"=\"*150)\n",
    "    for idx, row in df_results_intensive.iterrows():\n",
    "        print(f\"\\n{row['Model']}:\")\n",
    "        params = eval(row['Best Params'])\n",
    "        for param, value in params.items():\n",
    "            print(f\"  - {param}: {value}\")\n",
    "    \n",
    "    # Display final rankings\n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"FINAL MODEL RANKINGS (BY TEST R2)\")\n",
    "    print(\"=\"*150)\n",
    "    final_ranking = df_results_intensive.sort_values('Test R2', ascending=False)[\n",
    "        ['Model', 'Test R2', 'Test RMSE', 'Test MAE', 'Test MAPE', 'R2 Improvement']\n",
    "    ]\n",
    "    print(final_ranking.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    \n",
    "    # Identify best overall model\n",
    "    best_model_name = df_results_intensive.loc[df_results_intensive['Test R2'].idxmax(), 'Model']\n",
    "    best_model_r2 = df_results_intensive['Test R2'].max()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"BEST OVERALL MODEL\")\n",
    "    print(\"=\"*150)\n",
    "    print(f\"Model: {best_model_name}\")\n",
    "    print(f\"Test R2: {best_model_r2:.4f}\")\n",
    "    print(\"=\"*150)\n",
    "    \n",
    "    return df_results_intensive, best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736bec3",
   "metadata": {},
   "source": [
    "Run the tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5faeb703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================================\n",
      "STAGE 1: QUICK HYPERPARAMETER TUNING - ALL 6 MODELS\n",
      "======================================================================================================================================================\n",
      "\n",
      "Performing quick hyperparameter tuning\n",
      "\n",
      "  Training CatBoost \n",
      "  Training XGBoost \n",
      "  Training Random Forest "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\envs\\flightP\\lib\\site-packages\\sklearn\\base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training Gradient Boosting "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\envs\\flightP\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training K-Neighbors \n",
      "  Training Decision Tree \n",
      "======================================================================================================================================================\n",
      "QUICK TUNING RESULTS - ALL MODELS\n",
      "======================================================================================================================================================\n",
      "            Model  Train RMSE  Train MSE  Train MAE  Train MAPE  Train R2  Test RMSE  Test MSE  Test MAE  Test MAPE  Test R2\n",
      "         CatBoost      0.1385     0.0192     0.1020      0.0113    0.9264     0.1561    0.0244    0.1168     0.0130   0.9084\n",
      "          XGBoost      0.1447     0.0209     0.1065      0.0118    0.9197     0.1591    0.0253    0.1196     0.0133   0.9049\n",
      "    Random Forest      0.1285     0.0165     0.0895      0.0099    0.9366     0.1780    0.0317    0.1250     0.0138   0.8810\n",
      "Gradient Boosting      0.1428     0.0204     0.1045      0.0116    0.9218     0.1598    0.0255    0.1193     0.0132   0.9041\n",
      "      K-Neighbors      0.1793     0.0321     0.1257      0.0139    0.8767     0.2026    0.0410    0.1412     0.0156   0.8459\n",
      "    Decision Tree      0.1325     0.0175     0.0858      0.0095    0.9327     0.1809    0.0327    0.1225     0.0136   0.8771\n",
      "\n",
      "======================================================================================================================================================\n",
      "TOP 3 MODELS SELECTED FOR INTENSIVE TUNING\n",
      "======================================================================================================================================================\n",
      "1. CatBoost\n",
      "   - Test R2: 0.9084\n",
      "   - Test RMSE: 0.1561\n",
      "   - Test MAE: 0.1168\n",
      "2. XGBoost\n",
      "   - Test R2: 0.9049\n",
      "   - Test RMSE: 0.1591\n",
      "   - Test MAE: 0.1196\n",
      "3. Gradient Boosting\n",
      "   - Test R2: 0.9041\n",
      "   - Test RMSE: 0.1598\n",
      "   - Test MAE: 0.1193\n"
     ]
    }
   ],
   "source": [
    "# Run Stage 1\n",
    "df_quick, trained_models_quick, top_3 = flight_price_quick_hyperparam_tuning(\n",
    "    scaled_X_train, scaled_y_train, scaled_X_test, scaled_y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ad5fbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================================================================================================\n",
      "STAGE 2: Complete HYPERPARAMETER TUNING - TOP 3 MODELS\n",
      "======================================================================================================================================================\n",
      "\n",
      "Performing intensive hyperparameter tuning on top 3 models\n",
      "\n",
      "  Training CatBoost Completed\n",
      "\n",
      "  Training XGBoost Completed\n",
      "\n",
      "  Training Gradient Boosting "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\envs\\flightP\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "\n",
      "======================================================================================================================================================\n",
      "Complete TUNING RESULTS - TOP 3 MODELS\n",
      "======================================================================================================================================================\n",
      "            Model  Train RMSE  Train MSE  Train MAE  Train MAPE  Train R2  Test RMSE  Test MSE  Test MAE  Test MAPE  Test R2  R2 Improvement\n",
      "         CatBoost      0.1346     0.0181     0.0978      0.0109    0.9305     0.1537    0.0236    0.1136     0.0126   0.9113          0.0028\n",
      "          XGBoost      0.1361     0.0185     0.0988      0.0110    0.9290     0.1547    0.0239    0.1139     0.0126   0.9101          0.0053\n",
      "Gradient Boosting      0.1228     0.0151     0.0852      0.0095    0.9422     0.1542    0.0238    0.1101     0.0122   0.9107          0.0066\n",
      "\n",
      "======================================================================================================================================================\n",
      "BEST HYPERPARAMETERS - Complete TUNING\n",
      "======================================================================================================================================================\n",
      "\n",
      "CatBoost:\n",
      "  - subsample: 0.9\n",
      "  - learning_rate: 0.1\n",
      "  - l2_leaf_reg: 1\n",
      "  - iterations: 500\n",
      "  - depth: 6\n",
      "  - border_count: 64\n",
      "\n",
      "XGBoost:\n",
      "  - subsample: 0.8\n",
      "  - reg_lambda: 1\n",
      "  - reg_alpha: 1\n",
      "  - n_estimators: 300\n",
      "  - min_child_weight: 3\n",
      "  - max_depth: 7\n",
      "  - learning_rate: 0.05\n",
      "  - gamma: 0\n",
      "  - colsample_bytree: 0.7\n",
      "\n",
      "Gradient Boosting:\n",
      "  - subsample: 1.0\n",
      "  - n_estimators: 500\n",
      "  - min_samples_split: 5\n",
      "  - min_samples_leaf: 4\n",
      "  - max_features: None\n",
      "  - max_depth: 7\n",
      "  - loss: squared_error\n",
      "  - learning_rate: 0.03\n",
      "\n",
      "======================================================================================================================================================\n",
      "FINAL MODEL RANKINGS (BY TEST R2)\n",
      "======================================================================================================================================================\n",
      "            Model  Test R2  Test RMSE  Test MAE  Test MAPE  R2 Improvement\n",
      "         CatBoost   0.9113     0.1537    0.1136     0.0126          0.0028\n",
      "Gradient Boosting   0.9107     0.1542    0.1101     0.0122          0.0066\n",
      "          XGBoost   0.9101     0.1547    0.1139     0.0126          0.0053\n",
      "\n",
      "======================================================================================================================================================\n",
      "BEST OVERALL MODEL\n",
      "======================================================================================================================================================\n",
      "Model: CatBoost\n",
      "Test R2: 0.9113\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run Stage 2 with top 3 models from Stage 1\n",
    "df_intensive, best_models = flight_price_complete_hyperparam_tuning(\n",
    "    scaled_X_train, scaled_y_train, scaled_X_test, scaled_y_test, top_3, df_quick\n",
    ")\n",
    "\n",
    "# Access the best model\n",
    "# best_model = best_models['CatBoost']  # Replace with the actual best model name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e562c",
   "metadata": {},
   "source": [
    "CatBoost is best overall model with these hyperparams."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flightP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
