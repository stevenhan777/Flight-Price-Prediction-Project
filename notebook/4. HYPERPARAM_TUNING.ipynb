{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37b21a8",
   "metadata": {},
   "source": [
    "##### Hyperparam Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e629d4",
   "metadata": {},
   "source": [
    "Here I will do hyperparameter tuning on the best models on the scaled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db249a",
   "metadata": {},
   "source": [
    "Import required packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d6e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651f9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge,Lasso\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a8670",
   "metadata": {},
   "source": [
    "Read in the scaled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02db8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train=pd.read_csv('data/scaled_X_train.csv')\n",
    "scaled_y_train=pd.read_csv('data/scaled_y_train.csv')\n",
    "scaled_X_test=pd.read_csv('data/scaled_X_test.csv')\n",
    "scaled_y_test=pd.read_csv('data/scaled_y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f460fd28",
   "metadata": {},
   "source": [
    "Define evalute_model function as done in the Model notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22599560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true, predicted):\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(true, predicted))\n",
    "    mape = mean_absolute_percentage_error(true, predicted)\n",
    "    r2_square = r2_score(true, predicted)\n",
    "    return mae, mse, rmse, mape, r2_square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a481307f",
   "metadata": {},
   "source": [
    "##### These are the following hyperparameters that I use:\n",
    "\n",
    "CatBoost\n",
    "* 'iterations': [100, 200], - total number of decision trees built sequentially during the training process\n",
    "* 'depth': [4, 6, 8], - defining the maximum number of levels a tree can grow\n",
    "* 'learning_rate': [0.05, 0.1], - It scales the contribution of each newly added decision tree to the final model's prediction\n",
    "* 'l2_leaf_reg': [1, 3, 5] - L2 regularization parameter that adds a penalty to the leaf values in the cost function\n",
    "\n",
    "XGBoost\n",
    "* 'n_estimators': [100, 200], - maximum number of individual decision trees\n",
    "* 'max_depth': [3, 5, 7], - maximum number of splits each decision tree in the ensemble can grow to\n",
    "* 'learning_rate': [0.05, 0.1], - After a new tree is built to correct errors, its predictions are multiplied by the learning_rate before being added to the overall model\n",
    "* 'subsample': [0.8, 1.0], - value like 0.5 means that XGBoost will randomly sample 50% of the total training data\n",
    "* 'colsample_bytree': [0.8, 1.0] - controls the fraction of features that are randomly selected for use in a single decision tree\n",
    "\n",
    "Random Forest\n",
    "* 'n_estimators': [100, 200], - number of decision trees to build\n",
    "* 'max_depth': [10, 20, None], - maximum number of splits in each individual decision tree\n",
    "* 'min_samples_split': [2, 5], - minimum number of samples required to split an internal node\n",
    "* 'min_samples_leaf': [1, 2], - minimum number of samples required to be present in a leaf node\n",
    "* 'max_features': ['sqrt', 'log2'] - maximum number of features at each individual node\n",
    "\n",
    "Gradient Boosting \n",
    "* 'n_estimators': [100, 200], - number of individual decision trees that are built sequentially to form the final ensemble model\n",
    "* 'max_depth': [3, 5, 7], - maximum depth or number of levels for each individual decision tree\n",
    "* 'learning_rate': [0.05, 0.1], - scales the contribution of each new, weak tree added to the ensemble\n",
    "* 'subsample': [0.8, 1.0], - introduces randomness by training each new tree on a random fraction of the original training data\n",
    "* 'min_samples_split': [2, 5] - minimum number of samples required in an internal node for it to be considered for a further split\n",
    "\n",
    "K-Neighbors\n",
    "* 'n_neighbors': [3, 5, 7, 9] - user-defined number of closest data points (neighbors) used to predict the value of a new point\n",
    "* 'weights': ['uniform', 'distance'] - how much influence each of the 'k' neighbors has on the final prediction\n",
    "    * uniform:  All k neighbors contribute equally\n",
    "    * distance: Each neighbor's value is weighted by the inverse of its distance to the query point (1/distance)\n",
    "* 'algorithm': ['auto', 'ball_tree', 'kd_tree'], - determines the method used to compute the nearest neighbors efficiently, particularly as dataset size increases\n",
    "    * auto: automatically choose\n",
    "    * ball_tree: Uses the BallTree data structure\n",
    "    * kd_tree: Uses the KDTree data structure\n",
    "* 'leaf_size': [20, 30, 40] -  control the size of the leaves in the data structure\n",
    "\n",
    "Decision Tree\n",
    "* 'max_depth': [5, 10, 15, 20, None] - length of the longest path from the root node to any leaf node\n",
    "* 'min_samples_split': [2, 5, 10] - the minimum number of samples an internal node must contain before it can be considered for splitting further. Used as a stopping criterion to control the growth of the tree and prevent overfitting\n",
    "* 'min_samples_leaf': [1, 2, 4] - the minimum number of samples that must be present in a node for it to be considered a valid leaf node. Used to prevent overfitting by ensuring that decisions are based on a sufficient number of data points\n",
    "* 'max_features': ['sqrt', 'log2', None] - randomly selects a subset of features from the total available features in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511688f4",
   "metadata": {},
   "source": [
    "Define the hyperparam tuning function to do first quick tuning on the 6 models, then more fine tuning for top 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b69049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flight_price_quick_hyperparam_tuning(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    print(\"=\"*150)\n",
    "    print(\"STAGE 1: QUICK HYPERPARAMETER TUNING - ALL 6 MODELS\")\n",
    "    print(\"=\"*150)\n",
    "    \n",
    "    # Stage 1: Quick hyperparameter tuning\n",
    "    quick_models = {\n",
    "        \"CatBoost\": {\n",
    "            \"model\": CatBoostRegressor(random_state=42, verbose=0),\n",
    "            \"params\": {\n",
    "                'iterations': [100, 200],\n",
    "                'depth': [4, 6, 8],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'l2_leaf_reg': [1, 3, 5]\n",
    "            }\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"model\": XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'subsample': [0.8, 1.0],\n",
    "                'colsample_bytree': [0.8, 1.0]\n",
    "            }\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            \"model\": RandomForestRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2],\n",
    "                'max_features': ['sqrt', 'log2']\n",
    "            }\n",
    "        },\n",
    "        \"Gradient Boosting\": {\n",
    "            \"model\": GradientBoostingRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'subsample': [0.8, 1.0],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        \"K-Neighbors\": {\n",
    "            \"model\": KNeighborsRegressor(),\n",
    "            \"params\": {\n",
    "                'n_neighbors': [3, 5, 7, 9],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "                'leaf_size': [20, 30, 40]\n",
    "            }\n",
    "        },\n",
    "        \"Decision Tree\": {\n",
    "            \"model\": DecisionTreeRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                'max_depth': [5, 10, 15, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2', None]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_quick = []\n",
    "    trained_models = {}\n",
    "    \n",
    "    # Quick tuning with fewer iterations\n",
    "    print(\"\\nPerforming quick hyperparameter tuning\")\n",
    "    for model_name, model_info in quick_models.items():\n",
    "        print(f\"\\n  Training {model_name}\", end=\" \")\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model_info[\"model\"],\n",
    "            param_distributions=model_info[\"params\"],\n",
    "            n_iter=20, \n",
    "            cv=3, \n",
    "            scoring='neg_mean_squared_error',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_model = random_search.best_estimator_\n",
    "        trained_models[model_name] = best_model\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        \n",
    "\n",
    "        model_train_mae, model_train_mse, model_train_rmse, model_train_mape, model_train_r2 = evaluate_model(y_train, y_train_pred)\n",
    "        model_test_mae, model_test_mse, model_test_rmse, model_test_mape, model_test_r2 = evaluate_model(y_test, y_test_pred)\n",
    "        \n",
    "        results_quick.append({\n",
    "            'Model': model_name,\n",
    "            'Train RMSE': model_train_rmse,\n",
    "            'Train MSE': model_train_mse,\n",
    "            'Train MAE': model_train_mae,\n",
    "            'Train MAPE': model_train_mape,\n",
    "            'Train R2': model_train_r2,\n",
    "            'Test RMSE': model_test_rmse,\n",
    "            'Test MSE': model_test_mse,\n",
    "            'Test MAE': model_test_mae,\n",
    "            'Test MAPE': model_test_mape,\n",
    "            'Test R2': model_test_r2,\n",
    "            'Best Params': str(random_search.best_params_)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for quick results\n",
    "    df_results_quick = pd.DataFrame(results_quick)\n",
    "    \n",
    "    # Display quick tuning results\n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"QUICK TUNING RESULTS - ALL MODELS\")\n",
    "    print(\"=\"*150)\n",
    "    display_df = df_results_quick.drop('Best Params', axis=1)\n",
    "    print(display_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    \n",
    "    # Identify top 3 models based on Test R2\n",
    "    top_3_models = df_results_quick.nlargest(3, 'Test R2')['Model'].tolist()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"TOP 3 MODELS SELECTED FOR INTENSIVE TUNING\")\n",
    "    print(\"=\"*150)\n",
    "    for i, model_name in enumerate(top_3_models, 1):\n",
    "        model_stats = df_results_quick[df_results_quick['Model'] == model_name].iloc[0]\n",
    "        print(f\"{i}. {model_name}\")\n",
    "        print(f\"   - Test R2: {model_stats['Test R2']:.4f}\")\n",
    "        print(f\"   - Test RMSE: {model_stats['Test RMSE']:.4f}\")\n",
    "        print(f\"   - Test MAE: {model_stats['Test MAE']:.4f}\")\n",
    "    \n",
    "    return df_results_quick, trained_models, top_3_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355fba76",
   "metadata": {},
   "source": [
    "Define more Intensive hyperparam tuning for the top 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flight_price_complete_hyperparam_tuning(X_train, y_train, X_test, y_test, top_3_models, df_results_quick):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"STAGE 2: Complete HYPERPARAMETER TUNING - TOP 3 MODELS\")\n",
    "    print(\"=\"*150)\n",
    "    \n",
    "    intensive_models = {\n",
    "        \"CatBoost\": {\n",
    "            \"model\": CatBoostRegressor(random_state=42, verbose=0),\n",
    "            \"params\": {\n",
    "                'iterations': [100, 200, 300, 500],\n",
    "                'depth': [4, 6, 8, 10],\n",
    "                'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15],\n",
    "                'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "                'border_count': [32, 64, 128],\n",
    "                'subsample': [0.7, 0.8, 0.9, 1.0]\n",
    "            }\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"model\": XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200, 300, 500],\n",
    "                'max_depth': [3, 5, 7, 9, 11],\n",
    "                'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15],\n",
    "                'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                'gamma': [0, 0.1, 0.3, 0.5, 1],\n",
    "                'min_child_weight': [1, 3, 5, 7],\n",
    "                'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "                'reg_lambda': [0.1, 1, 10]\n",
    "            }\n",
    "        },\n",
    "        \"Gradient Boosting\": {\n",
    "            \"model\": GradientBoostingRegressor(random_state=42),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200, 300, 500],\n",
    "                'max_depth': [3, 5, 7, 9, 11],\n",
    "                'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15],\n",
    "                'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                'min_samples_split': [2, 5, 10, 15],\n",
    "                'min_samples_leaf': [1, 2, 4, 6],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'loss': ['squared_error', 'absolute_error', 'huber']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_intensive = []\n",
    "    best_models = {}\n",
    "    \n",
    "    for model_name in top_3_models:\n",
    "        if model_name not in intensive_models:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n  Training {model_name}\", end=\" \")\n",
    "        \n",
    "        model_info = intensive_models[model_name]\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model_info[\"model\"],\n",
    "            param_distributions=model_info[\"params\"],\n",
    "            n_iter=50,  \n",
    "            cv=5,  \n",
    "            scoring='neg_mean_squared_error',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_model = random_search.best_estimator_\n",
    "        best_models[model_name] = best_model\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        \n",
    "        model_train_mae, model_train_mse, model_train_rmse, model_train_mape, model_train_r2 = evaluate_model(y_train, y_train_pred)\n",
    "        model_test_mae, model_test_mse, model_test_rmse, model_test_mape, model_test_r2 = evaluate_model(y_test, y_test_pred)\n",
    "        \n",
    "        # Calculate improvement from quick tuning\n",
    "        quick_r2 = df_results_quick[df_results_quick['Model'] == model_name]['Test R2'].values[0]\n",
    "        r2_improvement = model_test_r2 - quick_r2\n",
    "        \n",
    "        print(\"Completed\")\n",
    "        \n",
    "        results_intensive.append({\n",
    "            'Model': model_name,\n",
    "            'Train RMSE': model_train_rmse,\n",
    "            'Train MSE': model_train_mse,\n",
    "            'Train MAE': model_train_mae,\n",
    "            'Train MAPE': model_train_mape,\n",
    "            'Train R2': model_train_r2,\n",
    "            'Test RMSE': model_test_rmse,\n",
    "            'Test MSE': model_test_mse,\n",
    "            'Test MAE': model_test_mae,\n",
    "            'Test MAPE': model_test_mape,\n",
    "            'Test R2': model_test_r2,\n",
    "            'R2 Improvement': r2_improvement,\n",
    "            'Best Params': str(random_search.best_params_)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for intensive results\n",
    "    df_results_intensive = pd.DataFrame(results_intensive)\n",
    "    \n",
    "    # Display intensive tuning results\n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"Complete TUNING RESULTS - TOP 3 MODELS\")\n",
    "    print(\"=\"*150)\n",
    "    display_df_intensive = df_results_intensive.drop('Best Params', axis=1)\n",
    "    print(display_df_intensive.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    \n",
    "    # Display best parameters for intensive tuning\n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"BEST HYPERPARAMETERS - Complete TUNING\")\n",
    "    print(\"=\"*150)\n",
    "    for idx, row in df_results_intensive.iterrows():\n",
    "        print(f\"\\n{row['Model']}:\")\n",
    "        params = eval(row['Best Params'])\n",
    "        for param, value in params.items():\n",
    "            print(f\"  - {param}: {value}\")\n",
    "    \n",
    "    # Display final rankings\n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"FINAL MODEL RANKINGS (BY TEST R2)\")\n",
    "    print(\"=\"*150)\n",
    "    final_ranking = df_results_intensive.sort_values('Test R2', ascending=False)[\n",
    "        ['Model', 'Test R2', 'Test RMSE', 'Test MAE', 'Test MAPE', 'R2 Improvement']\n",
    "    ]\n",
    "    print(final_ranking.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    \n",
    "    # Identify best overall model\n",
    "    best_model_name = df_results_intensive.loc[df_results_intensive['Test R2'].idxmax(), 'Model']\n",
    "    best_model_r2 = df_results_intensive['Test R2'].max()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*150)\n",
    "    print(\"BEST OVERALL MODEL\")\n",
    "    print(\"=\"*150)\n",
    "    print(f\"Model: {best_model_name}\")\n",
    "    print(f\"Test R2: {best_model_r2:.4f}\")\n",
    "    print(\"=\"*150)\n",
    "    \n",
    "    return df_results_intensive, best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736bec3",
   "metadata": {},
   "source": [
    "Run the quick hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5faeb703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================================\n",
      "STAGE 1: QUICK HYPERPARAMETER TUNING - ALL 6 MODELS\n",
      "======================================================================================================================================================\n",
      "\n",
      "Performing quick hyperparameter tuning\n",
      "\n",
      "  Training CatBoost \n",
      "  Training XGBoost \n",
      "  Training Random Forest "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\envs\\flightP311\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training Gradient Boosting "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\envs\\flightP311\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training K-Neighbors \n",
      "  Training Decision Tree \n",
      "======================================================================================================================================================\n",
      "QUICK TUNING RESULTS - ALL MODELS\n",
      "======================================================================================================================================================\n",
      "            Model  Train RMSE  Train MSE  Train MAE  Train MAPE  Train R2  Test RMSE  Test MSE  Test MAE  Test MAPE  Test R2\n",
      "         CatBoost      0.1385     0.0192     0.1020      0.0113    0.9264     0.1561    0.0244    0.1168     0.0130   0.9084\n",
      "          XGBoost      0.1447     0.0209     0.1065      0.0118    0.9197     0.1591    0.0253    0.1196     0.0133   0.9049\n",
      "    Random Forest      0.1285     0.0165     0.0895      0.0099    0.9366     0.1780    0.0317    0.1250     0.0138   0.8810\n",
      "Gradient Boosting      0.1428     0.0204     0.1045      0.0116    0.9218     0.1598    0.0255    0.1193     0.0132   0.9041\n",
      "      K-Neighbors      0.1793     0.0321     0.1257      0.0139    0.8767     0.2026    0.0410    0.1412     0.0156   0.8459\n",
      "    Decision Tree      0.1325     0.0175     0.0858      0.0095    0.9327     0.1809    0.0327    0.1225     0.0136   0.8771\n",
      "\n",
      "======================================================================================================================================================\n",
      "TOP 3 MODELS SELECTED FOR INTENSIVE TUNING\n",
      "======================================================================================================================================================\n",
      "1. CatBoost\n",
      "   - Test R2: 0.9084\n",
      "   - Test RMSE: 0.1561\n",
      "   - Test MAE: 0.1168\n",
      "2. XGBoost\n",
      "   - Test R2: 0.9049\n",
      "   - Test RMSE: 0.1591\n",
      "   - Test MAE: 0.1196\n",
      "3. Gradient Boosting\n",
      "   - Test R2: 0.9041\n",
      "   - Test RMSE: 0.1598\n",
      "   - Test MAE: 0.1193\n"
     ]
    }
   ],
   "source": [
    "df_quick, trained_models_quick, top_3 = flight_price_quick_hyperparam_tuning(\n",
    "    scaled_X_train, scaled_y_train, scaled_X_test, scaled_y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea81826",
   "metadata": {},
   "source": [
    "Observations\n",
    "Top 3 models based of test R2 score is Catboost, XGboost and Gradient boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92635698",
   "metadata": {},
   "source": [
    "Run the intensive hyperparam tuning on the top 3 models from stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ad5fbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================================================================================================\n",
      "STAGE 2: Complete HYPERPARAMETER TUNING - TOP 3 MODELS\n",
      "======================================================================================================================================================\n",
      "\n",
      "  Training CatBoost Completed\n",
      "\n",
      "  Training XGBoost Completed\n",
      "\n",
      "  Training Gradient Boosting "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\envs\\flightP311\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "\n",
      "======================================================================================================================================================\n",
      "Complete TUNING RESULTS - TOP 3 MODELS\n",
      "======================================================================================================================================================\n",
      "            Model  Train RMSE  Train MSE  Train MAE  Train MAPE  Train R2  Test RMSE  Test MSE  Test MAE  Test MAPE  Test R2  R2 Improvement\n",
      "         CatBoost      0.1346     0.0181     0.0978      0.0109    0.9305     0.1537    0.0236    0.1136     0.0126   0.9113          0.0028\n",
      "          XGBoost      0.1361     0.0185     0.0988      0.0110    0.9290     0.1547    0.0239    0.1139     0.0126   0.9101          0.0053\n",
      "Gradient Boosting      0.1228     0.0151     0.0852      0.0095    0.9422     0.1542    0.0238    0.1101     0.0122   0.9107          0.0066\n",
      "\n",
      "======================================================================================================================================================\n",
      "BEST HYPERPARAMETERS - Complete TUNING\n",
      "======================================================================================================================================================\n",
      "\n",
      "CatBoost:\n",
      "  - subsample: 0.9\n",
      "  - learning_rate: 0.1\n",
      "  - l2_leaf_reg: 1\n",
      "  - iterations: 500\n",
      "  - depth: 6\n",
      "  - border_count: 64\n",
      "\n",
      "XGBoost:\n",
      "  - subsample: 0.8\n",
      "  - reg_lambda: 1\n",
      "  - reg_alpha: 1\n",
      "  - n_estimators: 300\n",
      "  - min_child_weight: 3\n",
      "  - max_depth: 7\n",
      "  - learning_rate: 0.05\n",
      "  - gamma: 0\n",
      "  - colsample_bytree: 0.7\n",
      "\n",
      "Gradient Boosting:\n",
      "  - subsample: 1.0\n",
      "  - n_estimators: 500\n",
      "  - min_samples_split: 5\n",
      "  - min_samples_leaf: 4\n",
      "  - max_features: None\n",
      "  - max_depth: 7\n",
      "  - loss: squared_error\n",
      "  - learning_rate: 0.03\n",
      "\n",
      "======================================================================================================================================================\n",
      "FINAL MODEL RANKINGS (BY TEST R2)\n",
      "======================================================================================================================================================\n",
      "            Model  Test R2  Test RMSE  Test MAE  Test MAPE  R2 Improvement\n",
      "         CatBoost   0.9113     0.1537    0.1136     0.0126          0.0028\n",
      "Gradient Boosting   0.9107     0.1542    0.1101     0.0122          0.0066\n",
      "          XGBoost   0.9101     0.1547    0.1139     0.0126          0.0053\n",
      "\n",
      "======================================================================================================================================================\n",
      "BEST OVERALL MODEL\n",
      "======================================================================================================================================================\n",
      "Model: CatBoost\n",
      "Test R2: 0.9113\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "df_intensive, best_models = flight_price_complete_hyperparam_tuning(\n",
    "    scaled_X_train, scaled_y_train, scaled_X_test, scaled_y_test, top_3, df_quick\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6dbf7b",
   "metadata": {},
   "source": [
    "Observation\n",
    "\n",
    "* The best model after hyperparameter tuning is Catboost model with Test R2 of 0.9113\n",
    "* Gradient boosting and XGBoost models follow and are very similar in performance\n",
    "* The params for Catboost used are\n",
    "    * subsample: 0.9\n",
    "    * learning_rate: 0.1\n",
    "    * l2_leaf_reg: 1\n",
    "    * iterations: 500\n",
    "    * depth: 6\n",
    "    * border_count: 64\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320d730",
   "metadata": {},
   "source": [
    "For catboost model\n",
    "* Note that maximum number of possible iterations are used (500)\n",
    "* The depth is tuned to 6 between 4 to 10 indicating try to prevent overfitting\n",
    "* L2 regularization is set to 1 indicating standard regurlization\n",
    "* subsample set to 0.9 between 0.7 and 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flightP311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
